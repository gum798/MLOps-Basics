
Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]
  | Name | Type      | Params
-----------------------------------
0 | bert | BertModel | 4.4 M
1 | W    | Linear    | 258
-----------------------------------
4.4 M     Trainable params
0         Non-trainable params
4.4 M     Total params
17.545    Total estimated model params size (MB)
C:\Users\jhseo\Anaconda3\envs\mlops\lib\site-packages\pytorch_lightning\trainer\data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.

Validation sanity check: 100%|██████████| 2/2 [00:03<00:00,  1.72s/it]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
C:\Users\jhseo\Anaconda3\envs\mlops\lib\site-packages\pytorch_lightning\core\datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  rank_zero_deprecation(
  | Name | Type      | Params
-----------------------------------
0 | bert | BertModel | 4.4 M
1 | W    | Linear    | 258
-----------------------------------
4.4 M     Trainable params
0         Non-trainable params
4.4 M     Total params
17.545    Total estimated model params size (MB)
C:\Users\jhseo\Anaconda3\envs\mlops\lib\site-packages\pytorch_lightning\trainer\data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.


Validation sanity check: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
C:\Users\jhseo\Anaconda3\envs\mlops\lib\site-packages\pytorch_lightning\core\datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  rank_zero_deprecation(
  | Name | Type      | Params
-----------------------------------
0 | bert | BertModel | 4.4 M
1 | W    | Linear    | 258
-----------------------------------
4.4 M     Trainable params
0         Non-trainable params
4.4 M     Total params
17.545    Total estimated model params size (MB)
C:\Users\jhseo\Anaconda3\envs\mlops\lib\site-packages\pytorch_lightning\trainer\data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.


Validation sanity check: 100%|██████████| 2/2 [00:02<00:00,  1.40s/it]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
C:\Users\jhseo\Anaconda3\envs\mlops\lib\site-packages\pytorch_lightning\core\datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  rank_zero_deprecation(
  | Name | Type      | Params
-----------------------------------
0 | bert | BertModel | 4.4 M
1 | W    | Linear    | 258
-----------------------------------
4.4 M     Trainable params
0         Non-trainable params
4.4 M     Total params
17.545    Total estimated model params size (MB)
C:\Users\jhseo\Anaconda3\envs\mlops\lib\site-packages\pytorch_lightning\trainer\data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.


Validation sanity check: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]
GPU available: False, used: False
TPU available: False, using: 0 TPU cores

Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s].08s/it]
C:\Users\jhseo\Anaconda3\envs\mlops\lib\site-packages\pytorch_lightning\core\datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  rank_zero_deprecation(
  | Name | Type      | Params
-----------------------------------
0 | bert | BertModel | 4.4 M
1 | W    | Linear    | 258
-----------------------------------
4.4 M     Trainable params
0         Non-trainable params
4.4 M     Total params
17.545    Total estimated model params size (MB)
C:\Users\jhseo\Anaconda3\envs\mlops\lib\site-packages\pytorch_lightning\trainer\data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.



Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google/bert_uncased_L-2_H-128_A-2 and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
C:\Users\jhseo\Anaconda3\envs\mlops\lib\site-packages\pytorch_lightning\callbacks\model_checkpoint.py:446: UserWarning: Checkpoint directory ./models exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Reusing dataset glue (C:\Users\jhseo\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
100%|██████████| 3/3 [00:00<00:00, 272.73it/s]
  0%|          | 0/3 [00:00<?, ?it/s]

100%|██████████| 2/2 [00:00<00:00,  5.13ba/s]
 50%|█████     | 1/2 [00:00<00:00,  2.75ba/s]
  | Name                   | Type                          | Params
-------------------------------------------------------------------------
0 | bert                   | BertForSequenceClassification | 4.4 M
1 | train_accuracy_metric  | Accuracy                      | 0
2 | val_accuracy_metric    | Accuracy                      | 0
3 | f1_metric              | F1                            | 0
4 | precision_macro_metric | Precision                     | 0
5 | recall_macro_metric    | Recall                        | 0
6 | precision_micro_metric | Precision                     | 0
7 | recall_micro_metric    | Recall                        | 0
-------------------------------------------------------------------------
4.4 M     Trainable params
0         Non-trainable params
4.4 M     Total params
17.545    Total estimated model params size (MB)
C:\Users\jhseo\Anaconda3\envs\mlops\lib\site-packages\pytorch_lightning\trainer\data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(

