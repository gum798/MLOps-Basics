{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "#!conda install tensorflow\r\n",
    "#!conda install pytorch\r\n",
    "#!conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge\r\n",
    "#!pip install pytorch-lightning\r\n",
    "#!conda install -c conda-forge pytorch-lightning\r\n",
    "#!pip install datasets\r\n",
    "#!pip install transformers\r\n",
    "#!conda install keras\r\n",
    "#!pip install wandb\r\n",
    "#!pip install sklearn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from transformers import AutoTokenizer, AutoModel\r\n",
    "import torchmetrics\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from pytorch_lightning.callbacks  import EarlyStopping, ModelCheckpoint\r\n",
    "from datasets import list_datasets, load_dataset\r\n",
    "import pytorch_lightning as pl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "\r\n",
    "datasets_list = list_datasets()\r\n",
    "len(datasets_list)\r\n",
    "print(', '.join(dataset for dataset in datasets_list))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "acronym_identification, ade_corpus_v2, adversarial_qa, aeslc, afrikaans_ner_corpus, ag_news, ai2_arc, air_dialogue, ajgt_twitter_ar, allegro_reviews, allocine, alt, amazon_polarity, amazon_reviews_multi, amazon_us_reviews, ambig_qa, ami, amttl, anli, app_reviews, aqua_rat, aquamuse, ar_cov19, ar_res_reviews, ar_sarcasm, arabic_billion_words, arabic_pos_dialect, arabic_speech_corpus, arcd, arsentd_lev, art, arxiv_dataset, ascent_kb, aslg_pc12, asnq, asset, assin, assin2, atomic, autshumato, babi_qa, banking77, bbaw_egyptian, bbc_hindi_nli, bc2gm_corpus, beans, best2009, bianet, bible_para, big_patent, billsum, bing_coronavirus_query_set, biomrc, biosses, blended_skill_talk, blimp, blog_authorship_corpus, bn_hate_speech, bookcorpus, bookcorpusopen, boolq, bprec, break_data, brwac, bsd_ja_en, bswac, c3, c4, cail2018, caner, capes, casino, catalonia_independence, cats_vs_dogs, cawac, cbt, cc100, cc_news, ccaligned_multilingual, cdsc, cdt, cedr, cfq, chr_en, cifar10, cifar100, circa, civil_comments, clickbait_news_bg, climate_fever, clinc_oos, clue, cmrc2018, cnn_dailymail, coached_conv_pref, coarse_discourse, codah, code_search_net, code_x_glue_cc_clone_detection_big_clone_bench, code_x_glue_cc_clone_detection_poj104, code_x_glue_cc_cloze_testing_all, code_x_glue_cc_cloze_testing_maxmin, code_x_glue_cc_code_completion_line, code_x_glue_cc_code_completion_token, code_x_glue_cc_code_refinement, code_x_glue_cc_code_to_code_trans, code_x_glue_cc_defect_detection, code_x_glue_ct_code_to_text, code_x_glue_tc_nl_code_search_adv, code_x_glue_tc_text_to_code, code_x_glue_tt_text_to_text, com_qa, common_gen, common_language, common_voice, commonsense_qa, competition_math, compguesswhat, conceptnet5, conll2000, conll2002, conll2003, conllpp, conv_ai, conv_ai_2, conv_ai_3, conv_questions, coqa, cord19, cornell_movie_dialog, cos_e, cosmos_qa, counter, covid_qa_castorini, covid_qa_deepset, covid_qa_ucsd, covid_tweets_japanese, covost2, craigslist_bargains, crawl_domain, crd3, crime_and_punish, crows_pairs, cryptonite, cs_restaurants, cuad, curiosity_dialogs, daily_dialog, dane, danish_political_comments, dart, datacommons_factcheck, dbpedia_14, dbrd, deal_or_no_dialog, definite_pronoun_resolution, dengue_filipino, dialog_re, diplomacy_detection, disaster_response_messages, discofuse, discovery, disfl_qa, doc2dial, docred, doqa, dream, drop, duorc, dutch_social, dyk, e2e_nlg, e2e_nlg_cleaned, ecb, ecthr_cases, eduge, ehealth_kd, eitb_parcc, eli5, emea, emo, emotion, emotone_ar, empathetic_dialogues, enriched_web_nlg, eraser_multi_rc, esnli, eth_py150_open, ethos, eu_regulatory_ir, eurlex, euronews, europa_eac_tm, europa_ecdc_tm, europarl_bilingual, event2Mind, evidence_infer_treatment, exams, factckbr, fake_news_english, fake_news_filipino, farsi_news, fashion_mnist, fever, few_rel, financial_phrasebank, finer, flores, flue, food101, fquad, freebase_qa, gap, gem, generated_reviews_enth, generics_kb, german_legal_entity_recognition, germaner, germeval_14, giga_fren, gigaword, glucose, glue, gnad10, go_emotions, gooaq, google_wellformed_query, grail_qa, great_code, guardian_authorship, gutenberg_time, hans, hansards, hard, harem, has_part, hate_offensive, hate_speech18, hate_speech_filipino, hate_speech_offensive, hate_speech_pl, hate_speech_portuguese, hatexplain, hausa_voa_ner, hausa_voa_topics, hda_nli_hindi, head_qa, health_fact, hebrew_projectbenyehuda, hebrew_sentiment, hebrew_this_world, hellaswag, hendrycks_test, hind_encorp, hindi_discourse, hippocorpus, hkcancor, hlgd, hope_edi, hotpot_qa, hover, hrenwac_para, hrwac, humicroedit, hybrid_qa, hyperpartisan_news_detection, iapp_wiki_qa_squad, id_clickbait, id_liputan6, id_nergrit_corpus, id_newspapers_2018, id_panl_bppt, id_puisi, igbo_english_machine_translation, igbo_monolingual, igbo_ner, ilist, imdb, imdb_urdu_reviews, imppres, indic_glue, indonlu, inquisitive_qg, interpress_news_category_tr, interpress_news_category_tr_lite, irc_disentangle, isixhosa_ner_corpus, isizulu_ner_corpus, iwslt2017, jeopardy, jfleg, jigsaw_toxicity_pred, jigsaw_unintended_bias, jnlpba, journalists_questions, kan_hope, kannada_news, kd_conv, kde4, kelm, kilt_tasks, kilt_wikipedia, kinnews_kirnews, klue, kor_3i4k, kor_hate, kor_ner, kor_nli, kor_nlu, kor_qpair, kor_sae, kor_sarcasm, labr, lama, lambada, large_spanish_corpus, laroseda, lc_quad, lener_br, liar, librispeech_asr, librispeech_lm, limit, lince, linnaeus, liveqa, lj_speech, lm1b, lst20, m_lama, mac_morpho, makhzan, masakhaner, math_dataset, math_qa, matinf, mbpp, mc4, mc_taco, md_gender_bias, mdd, med_hop, medal, medical_dialog, medical_questions_pairs, menyo20k_mt, meta_woz, metooma, metrec, miam, mkb, mkqa, mlqa, mlsum, mnist, mocha, moroco, movie_rationales, mrqa, ms_marco, ms_terms, msr_genomics_kbcomp, msr_sqa, msr_text_compression, msr_zhen_translation_parity, msra_ner, mt_eng_vietnamese, muchocine, multi_booked, multi_eurlex, multi_news, multi_nli, multi_nli_mismatch, multi_para_crawl, multi_re_qa, multi_woz_v22, multi_x_science_sum, mutual_friends, mwsc, myanmar_news, narrativeqa, narrativeqa_manual, natural_questions, ncbi_disease, nchlt, ncslgr, nell, neural_code_search, news_commentary, newsgroup, newsph, newsph_nli, newspop, newsqa, newsroom, nkjp-ner, nli_tr, nlu_evaluation_data, norec, norne, norwegian_ner, nq_open, nsmc, numer_sense, numeric_fused_head, oclar, offcombr, offenseval2020_tr, offenseval_dravidian, ofis_publik, ohsumed, ollie, omp, onestop_english, open_subtitles, openai_humaneval, openbookqa, openslr, openwebtext, opinosis, opus100, opus_books, opus_dgt, opus_dogc, opus_elhuyar, opus_euconst, opus_finlex, opus_fiskmo, opus_gnome, opus_infopankki, opus_memat, opus_montenegrinsubs, opus_openoffice, opus_paracrawl, opus_rf, opus_tedtalks, opus_ubuntu, opus_wikipedia, opus_xhosanavy, orange_sum, oscar, para_crawl, para_pat, parsinlu_reading_comprehension, paws, paws-x, pec, peer_read, peoples_daily_ner, per_sent, persian_ner, pg19, php, piaf, pib, piqa, pn_summary, poem_sentiment, polemo2, poleval2019_cyberbullying, poleval2019_mt, polsum, polyglot_ner, prachathai67k, pragmeval, proto_qa, psc, ptb_text_only, pubmed, pubmed_qa, py_ast, qa4mre, qa_srl, qa_zre, qangaroo, qanta, qasc, qasper, qed, qed_amara, quac, quail, quarel, quartz, quora, quoref, race, re_dial, reasoning_bg, recipe_nlg, reclor, reddit, reddit_tifu, refresd, reuters21578, ro_sent, ro_sts, ro_sts_parallel, roman_urdu, ronec, ropes, rotten_tomatoes, russian_super_glue, s2orc, samsum, sanskrit_classic, saudinewsnet, scan, scb_mt_enth_2020, schema_guided_dstc8, scicite, scielo, scientific_papers, scifact, sciq, scitail, scitldr, search_qa, sede, selqa, sem_eval_2010_task_8, sem_eval_2014_task_1, sem_eval_2018_task_1, sem_eval_2020_task_11, sent_comp, senti_lex, senti_ws, sentiment140, sepedi_ner, sesotho_ner_corpus, setimes, setswana_ner_corpus, sharc, sharc_modified, sick, silicone, simple_questions_v2, siswati_ner_corpus, smartdata, sms_spam, snips_built_in_intents, snli, snow_simplified_japanese_corpus, so_stacksample, social_bias_frames, social_i_qa, sofc_materials_articles, sogou_news, spanish_billion_words, spc, species_800, spider, squad, squad_adversarial, squad_es, squad_it, squad_kor_v1, squad_kor_v2, squad_v1_pt, squad_v2, squadshifts, srwac, sst, stereoset, stsb_mt_sv, stsb_multi_mt, style_change_detection, subjqa, super_glue, superb, swag, swahili, swahili_news, swda, swedish_medical_ner, swedish_ner_corpus, swedish_reviews, swiss_judgment_prediction, tab_fact, tamilmixsentiment, tanzil, tapaco, tashkeela, taskmaster1, taskmaster2, taskmaster3, tatoeba, ted_hrlr, ted_iwlst2013, ted_multi, ted_talks_iwslt, telugu_books, telugu_news, tep_en_fa_para, thai_toxicity_tweet, thainer, thaiqa_squad, thaisum, the_pile_books3, the_pile_openwebtext2, the_pile_stack_exchange, tilde_model, time_dial, times_of_india_news_headlines, timit_asr, tiny_shakespeare, tlc, tmu_gfm_dataset, totto, trec, trivia_qa, tsac, ttc4900, tunizi, tuple_ie, turk, turkish_movie_sentiment, turkish_ner, turkish_product_reviews, turkish_shrinked_ner, turku_ner_corpus, tweet_eval, tweet_qa, tweets_ar_en_parallel, tweets_hate_speech_detection, twi_text_c3, twi_wordsim353, tydiqa, ubuntu_dialogs_corpus, udhr, um005, un_ga, un_multi, un_pc, universal_dependencies, universal_morphologies, urdu_fake_news, urdu_sentiment_corpus, vivos, web_nlg, web_of_science, web_questions, weibo_ner, wi_locness, wiki40b, wiki_asp, wiki_atomic_edits, wiki_auto, wiki_bio, wiki_dpr, wiki_hop, wiki_lingua, wiki_movies, wiki_qa, wiki_qa_ar, wiki_snippets, wiki_source, wiki_split, wiki_summary, wikiann, wikicorpus, wikihow, wikipedia, wikisql, wikitext, wikitext_tl39, wili_2018, wino_bias, winograd_wsc, winogrande, wiqa, wisesight1000, wisesight_sentiment, wmt14, wmt15, wmt16, wmt17, wmt18, wmt19, wmt20_mlqe_task1, wmt20_mlqe_task2, wmt20_mlqe_task3, wmt_t2t, wnut_17, wongnai_reviews, woz_dialogue, wrbsc, x_stance, xcopa, xed_en_fi, xglue, xnli, xor_tydi_qa, xquad, xquad_r, xsum, xsum_factuality, xtreme, yahoo_answers_qa, yahoo_answers_topics, yelp_polarity, yelp_review_full, yoruba_bbc_topics, yoruba_gv_ner, yoruba_text_c3, yoruba_wordsim353, youtube_caption_corrections, zest, AConsApart/anime_subtitles_DialoGPT, Abdo1Kamr/Arabic_Hadith, AdWeeb/DravidianMT, Adnan/Urdu_News_Headlines, Akshith/aa, Akshith/g_rock, Akshith/test, Annielytics/DoctorsNotes, Avishekavi/Avi, BSC-TeMU/SQAC, BSC-TeMU/ancora-ca-ner, BSC-TeMU/sts-ca, BSC-TeMU/tecla, BSC-TeMU/viquiquad, BSC-TeMU/xquad-ca, Binbin/my_dataset, Bosio/pacman, Bosio/pacman_descriptions, CAGER/rick, CShorten/KerasBERT, CShorten/ZillowPrize, ChadxxxxHall/Inter-vision, Check/a_re_gi, Check/region_1, Check/region_2, Check/region_3, Check/region_4, Check/region_5, Check/region_6, Check/region_7, Check/region_8, Check/region_9, Check/regions, Check/vverify, Chun/dataset, CodedotAI/code_clippy, Cropinky/flatearther, Cropinky/rap_lyrics_english, Cropinky/wow_fishing_bobber, Cyberfish/pos_tagger, Cyberfish/text_error_correction, Darren/data, Davlan/conll2003_de_noMISC, Davlan/conll2003_noMISC, Davlan/masakhanerV1, EMBO/biolang, EMBO/sd-nlp, ESZER/H, Emon/sobuj, Enes3774/data, Eymen3455/xsum_tr, FRTNX/cosuju, FRTNX/worldbank-projects, Felix-ML/quoteli3, Firoj/CrisisBench, Francois/futures_es, Fraser/mnist-text-default, Fraser/mnist-text-no-spaces, Fraser/mnist-text-small, Fraser/news-category-dataset, Fraser/python-lines, Fraser/short-jokes, Fraser/wiki_sentences, Gabriel/squad_v2_sv, GalacticAI/Noirset, Gauravadlakha1509/new_one, Gwangho/NCBI-Sars-Cov-2, HUPD/hupd-test, Halilyesilceng/autonlp-data-nameEntityRecognition, HarleyQ/WitcherDialogue, Harveenchadha/bol-models, HarveyBWest/mybot, Hellisotherpeople/DebateSum, Husain/intent-classification-en-fr, IlyaGusev/gazeta, IlyaGusev/headline_cause, Intel/WEC-Eng, Jean-Baptiste/wikiner_fr, Jikiwa/demo1, Jikiwa/demo2, Jikiwa/demo3, Jikiwa/demo4, Jikiwa/stargazers, Jikiwa/temp-repo-valid, KBLab/suc3, KETI-AIR/aihub, KETI-AIR/klue, KETI-AIR/kor_corpora, KETI-AIR/korquad, KETI-AIR/nikl, Khanoooo/autonlp-data-Corona, LIAMF-USP/arc-retrieval-c4, LeoCordoba/CC-NEWS-ES-titles, LeoCordoba/CC-NEWS-ES, MBAH/MOVIESON, MKK/Dhivehi-English, MarianaSahagun/test, Mateo/testdataset, Melinoe/TheLabTexts, MickyMike/large_c_corpus, Mrleo1nid/Test_ru_dataset, Mulin/my_second_dataset, Mulin/my_third_dataset, NTUYG/RAGTest, Narsil/asr_dummy, Narsil/conversational_dummy, Narsil/image_dummy, NbAiLab/norec_agg, NbAiLab/norne, NbAiLab/norwegian_parliament, NbAiLab/test_corpus, Ofrit/tmp, Pengfei/test, Pongsaky/Wiki_SCG, Pyke/patent_abstract, QA/abk-eng, Remesita/tagged_reviews, RohanAiLab/persian_blog, RohanAiLab/persian_daily_news, RohanAiLab/persian_news_dataset, SCourthial/test, SajjadAyoubi/persian_qa, Sam2021/Arguement_Mining_CL2017, SaulLu/Natural_Questions_HTML, SaulLu/Natural_Questions_HTML_Toy, SaulLu/Natural_Questions_HTML_reduced_all, SaulLu/test, SaulLu/toy_struc_dataset, Serhii/Custom_SQuAD, ShinyQ/PPKM_Pemerintah, SoLID/shellcode_i_a32, SocialGrep/reddit-crypto-aug-2021, SocialGrep/reddit-nonewnormal-complete, SocialGrep/reddit-wallstreetbets-aug-2021, TRoboto/masc, Tatyana/ru_sentiment_dataset, Terry0107/RiSAWOZ, Tevatron/msmarco-passage-corpus, Tevatron/msmarco-passage, Tevatron/scifact-corpus, Tevatron/scifact, Tevatron/wikipedia-curated-corpus, Tevatron/wikipedia-curated, Tevatron/wikipedia-nq-corpus, Tevatron/wikipedia-nq, Tevatron/wikipedia-squad-corpus, Tevatron/wikipedia-squad, Tevatron/wikipedia-trivia-corpus, Tevatron/wikipedia-trivia, Tevatron/wikipedia-wq-corpus, Tevatron/wikipedia-wq, TheBlindBandit/SpongeNot, TimTreasure4/Test, Trainmaster9977/957, Trainmaster9977/zbakuman, Tyler/wikimatrix_collapsed, Valahaar/wsdmt, Vishva/UniFAQ_DataSET, VoVanPhuc/translate_vi2en, Wikidepia/IndoParaCrawl, Wikidepia/IndoSQuAD, Wikidepia/mc4-filter, WyrdCurt/AO4W, XiangPan/iflytek, XiangPan/snli_break, XiangXiang/clt, Yves/fhnw_swiss_parliament, Zaid/coqa_expanded, Zaid/quac_expanded, aapot/mc4_fi_cleaned, abhishek/autonlp-data-imdb_eval, abwicke/C-B-R, abwicke/koplo, adamlin/FewShotWoz, adamlin/companion, adamlin/daily_dialog, adamlin/domain_classification, adamlin/multiwoz_dst, adamlin/qa_verification, adamlin/roc_story, adamlin/rs, adamlin/weibo_ner, ajmbell/test-dataset, albertvillanova/datasets-tests-compression, albertvillanova/dummy_libri2mix, albertvillanova/tests-public-raw-jsonl, albertvillanova/tests-raw-jsonl, alireza655/alireza655, alittleie/mis_238, allegro/polish-question-passage-pairs, allegro/summarization-allegro-articles, allegro/summarization-polish-summaries-corpus, allenai/c4, allenai/scico, ami-wav2vec2/ami_headset_single_dummy, ami-wav2vec2/ami_headset_single_processed, anton-l/common_language, anton-l/superb_demo, anton-l/superb_dummy, anukaver/EstQA, artyeth/Dorian, ashish-shrivastava/dont-know-dataset, asi/wikitext_fr, astarostap/antisemitic-tweets, astarostap/antisemitic_tweets, atelders/politweets, athivvat/thai-rap-lyrics, ausgequetschtem/jtrddfhfgh, bavard/personachat_truecased, bemanningssitua/dplremjfjfj, berkergurcay/2020-10K-Reports, bertin-project/mc4-es-sampled, bertin-project/mc4-sampling, bhadresh-savani/web_split, bigscience/mc4-sampled, bigscience/open_subtitles_monolingual, blinoff/medical_qa_ru_data, braincode/braincode, bs-modeling-metadata/OSCAR_Entity_13_000, bs-modeling-metadata/c4_newslike_url_only, bs-modeling-metadata/website_metadata_c4, bsc/ancora-ca-ner, bsc/sts-ca, bsc/tecla, bsc/viquiquad, bsc/xquad-ca, caca/zscczs, cakiki/args_me, canwenxu/dogwhistle, castorini/mr-tydi-corpus, castorini/mr-tydi, caythuoc/caoduoclieu, ccccccc/hdjw_94ejrjr, cdleong/piglatin-mt, cdminix/iwslt2011, cdminix/mgb1, cemigo/taylor_vs_shakes, cemigo/test-data, chenyuxuan/wikigold, cheulyop/dementiabank, cheulyop/ksponspeech, clarin-pl/cst-wikinews, clarin-pl/kpwr-ner, clarin-pl/nkjp-pos, clarin-pl/polemo2-official, classla/copa_hr, classla/hr500k, classla/reldi_hr, classla/reldi_sr, classla/setimes_sr, clem/autonlp-data-french_word_detection, clips/mfaq, cnrcastroli/aaaa, coala/kkk, congpt/dstc23_asr, corypaik/prost, csebuetnlp/xlsum, csebuetnlp/xnli_bn, cstrathe435/Task2Dial, ctl/ConceptualCaptions, dalle-mini/encoded, dasago78/dasago78dataset, dataset/wikipedia_bn, david-wb/zeshel, deepset/germandpr, deepset/germanquad, dfgvhxfgv/fghghj, dfki-nlp/few-nerd, dgao/librispeech_nc_test, dgknrsln/Yorumsepeti, diiogo/brwac-clean, dispenst/jhghdghfd, dispix/test-dataset, dk-crazydiv/huggingface-modelhub, dongpil/test, dumitrescustefan/ronec, dvilasuero/ag_news_training_set_losses, dweb/squad_with_cola_scores, dynabench/dynasent, dynabench/qa, eason929/test, edfews/szdfcszdf, edsas/fgrdtgrdtdr, edsas/grttyi, ehcalabres/ravdess_speech, ejjaffe/onion_headlines_2_sources, eliza-dukim/load_klue_re, erikacardenas300/Zillow_Economics_Dataset, ervis/aaa, ervis/qqq, eugenesiow/BSD100, eugenesiow/Div2k, eugenesiow/PIRM, eugenesiow/Set14, eugenesiow/Set5, eugenesiow/Urban100, ewdrtfwe/54refyghrtf, fatvvs/autonlp-data-entity_model_conll2003, fihtrotuld/asu, flax-community/code_clippy_data, flax-community/conceptual-12m-mbart-50-multilingual, flax-community/conceptual-12m-multilingual-marian-128, flax-community/conceptual-12m-multilingual-marian-es, flax-community/conceptual-12m-multilingual-marian, flax-community/conceptual-captions-12, flax-community/dummy-oscar-als-32, flax-community/german-common-voice-processed, flax-community/german_common_crawl, flax-community/multilingual-vqa, flax-community/norwegian-clean-dummy, flax-community/swahili-safi, flax-sentence-embeddings/Gender_Bias_Evaluation_Set, flax-sentence-embeddings/paws-jsonl, flax-sentence-embeddings/stackexchange_math_jsonl, flax-sentence-embeddings/stackexchange_title_best_voted_answer_jsonl, flax-sentence-embeddings/stackexchange_title_body_jsonl, flax-sentence-embeddings/stackexchange_titlebody_best_and_down_voted_answer_jsonl, flax-sentence-embeddings/stackexchange_titlebody_best_voted_answer_jsonl, flax-sentence-embeddings/stackexchange_xml, formermagic/github_python_1m, formu/CVT, fulai/DuReader, fuliucansheng/data_for_test, fuliucansheng/minicoco, fuliucansheng/mininlp, fuliucansheng/pascal_voc, fuyun1107/clip-for-vlp, fvillena/cantemist, fvillena/spanish_diagnostics, gar1t/test, german-nlp-group/german_common_crawl, gmnlp/tico19, gpt3mix/rt20, gpt3mix/sst2, greeneggsandyaml/test-dataset-debug, gsarti/change_it, gsarti/clean_mc4_it, gsarti/flores_101, gsarti/itacola, gustavecortal/fr_covid_news, habu24/fdz, hakurei/touhou-dataset, hartzeer/kdfjdshfje, hf-internal-testing/cats_vs_dogs_sample, hf-internal-testing/fixtures_docvqa, hfface/poopi, holodata/sensai, howardmiddleton382/esuyertusutr, howardmiddleton382/wgweagwege, huggingFaceUser02/air21_grp13_inference_results, huggingFaceUser02/air21_grp13_tokenized_results, huggingartists/21-savage, huggingartists/25-17, huggingartists/50-cent, huggingartists/5nizza, huggingartists/5opka, huggingartists/aaron-watson, huggingartists/abba, huggingartists/adele, huggingartists/agata-christie, huggingartists/aikko, huggingartists/aimer, huggingartists/ajr, huggingartists/alan-walker, huggingartists/arash, huggingartists/arctic-monkeys, huggingartists/ariana-grande, huggingartists/armin-van-buuren, huggingartists/asdfgfa, huggingartists/asper-x, huggingartists/baklan, huggingartists/big-baby-tape, huggingartists/big-russian-boss, huggingartists/bill-wurtz, huggingartists/billie-eilish, huggingartists/billy-talent, huggingartists/bones, huggingartists/boris-grebenshikov, huggingartists/braii, huggingartists/bruce-springsteen, huggingartists/burzum, huggingartists/cardi-b, huggingartists/chester-bennington, huggingartists/cocomelon, huggingartists/coin, huggingartists/coldplay, huggingartists/dababy, huggingartists/david-bowie, huggingartists/ddt, huggingartists/deep-purple, huggingartists/denderty, huggingartists/dermot-kennedy, huggingartists/dj-artem-artemov, huggingartists/doja-cat, huggingartists/drake, huggingartists/dua-lipa, huggingartists/duran-duran, huggingartists/dzhizus, huggingartists/ed-sheeran, huggingartists/egor-kreed, huggingartists/egor-letov, huggingartists/elton-john, huggingartists/eminem, huggingartists/enigma, huggingartists/enya, huggingartists/epic-rap-battles-of-history, huggingartists/face, huggingartists/fascinoma, huggingartists/fear-factory, huggingartists/florence-the-machine, huggingartists/freelancer, huggingartists/galenskaparna-and-after-shave, huggingartists/ghost, huggingartists/ghostemane, huggingartists/ghostmane, huggingartists/gizmo, huggingartists/gorillaz, huggingartists/green-day, huggingartists/grigory-leps, huggingartists/grimes, huggingartists/gspd, huggingartists/gunna, huggingartists/hillsong-worship, huggingartists/i-dont-know-how-but-they-found-me, huggingartists/idktime, huggingartists/imagine-dragons, huggingartists/jah-khalib, huggingartists/jim-morrison, huggingartists/john-lennon, huggingartists/joji, huggingartists/joni-mitchell, huggingartists/justin-bieber, huggingartists/kanye-west, huggingartists/kasta, huggingartists/kehlani, huggingartists/king-krule, huggingartists/kipelov, huggingartists/kishlak, huggingartists/kizaru, huggingartists/krechet, huggingartists/krept-and-konan-bugzy-malone-sl-morisson-abra-cadabra-rv-and-snap-capone, huggingartists/lady-gaga, huggingartists/lazy-jay, huggingartists/led-zeppelin, huggingartists/lil-baby, huggingartists/lil-nas-x, huggingartists/lil-peep, huggingartists/lil-skies, huggingartists/lil-uzi-vert, huggingartists/linkin-park, huggingartists/little-big, huggingartists/lizer, huggingartists/logic, huggingartists/lorde, huggingartists/loud-luxury, huggingartists/loverance, huggingartists/lovv66, huggingartists/lumen, huggingartists/lyapis-trubetskoy, huggingartists/macan, huggingartists/machine-gun-kelly, huggingartists/madonna, huggingartists/maroon-5, huggingartists/mashina-vremeni, huggingartists/mating-ritual, huggingartists/max-korzh, huggingartists/mayot, huggingartists/melanie-martinez, huggingartists/metallica, huggingartists/mf-doom, huggingartists/michael-jackson, huggingartists/mikhail-gorshenev, huggingartists/miyagi, huggingartists/mnogoznaal, huggingartists/morgenshtern, huggingartists/mumiy-troll, huggingartists/muse, huggingartists/nervy, huggingartists/nirvana, huggingartists/noize-mc, huggingartists/obladaet, huggingartists/og-buda, huggingartists/ot-rus, huggingartists/our-last-night, huggingartists/oxxxymiron, huggingartists/peter-paul-and-mary, huggingartists/pharaoh, huggingartists/placebo, huggingartists/platina, huggingartists/pop-smoke, huggingartists/post-malone, huggingartists/pyrokinesis, huggingartists/queen, huggingartists/radiohead, huggingartists/rage-against-the-machine, huggingartists/red-hot-chili-peppers, huggingartists/rex-orange-county, huggingartists/rihanna, huggingartists/rocket, huggingartists/sam-kim, huggingartists/scriptonite, huggingartists/sektor-gaza, huggingartists/sergei-letov, huggingartists/sia, huggingartists/sid-sriram, huggingartists/skillet, huggingartists/slava-kpss, huggingartists/slava-marlow, huggingartists/snoop-dogg, huggingartists/sugar-ray, huggingartists/suicideoscope, huggingartists/sundara-karma, huggingartists/system-of-a-down, huggingartists/tanzy-minus, huggingartists/taylor-swift, huggingartists/the-69-eyes, huggingartists/the-avalanches, huggingartists/the-beatles, huggingartists/the-gazette, huggingartists/the-king-and-the-jester, huggingartists/the-the-pigs, huggingartists/the-velvet-underground, huggingartists/the-weeknd, huggingartists/tiamat, huggingartists/till-lindemann, huggingartists/tom-waits, huggingartists/tony-raut-and-garry-topor, huggingartists/tool, huggingartists/totpoc, huggingartists/travis-scott, huggingartists/twenty-one-pilots, huggingartists/upsahl, huggingartists/v-x-v-prince, huggingartists/van-morrison, huggingartists/veggietales, huggingartists/viktor-tsoi, huggingartists/vladimir-vysotsky, huggingartists/xxxtentacion, huggingartists/young-thug, huggingartists/yung-plague, huggingartists/zemfira, huggingface/label-files, huggingface-course/codeparrot-ds-train, huggingface-course/codeparrot-ds-valid, iamshsdf/sssssssssss, iarfmoose/question_generator, imthanhlv/binhvq_dedup, imthanhlv/binhvq_news21_raw, indonesian-nlp/id_personachat, jaimin/wav2vec2-large-xlsr-gujarati-demo, jakemarcus/MATH, jamol1741/test_dataset, jdepoix/junit_test_completion, jglaser/binding_affinity, jhqwqq/2, jimregan/clarinpl_sejmsenat, jimregan/clarinpl_studio, jimregan/foinse, jinmang2/load_klue_re, jinmang2/tacred_example, jiyoojeong/targetizer, jmamou/augmented-glue-sst2, joelito/ler, joelito/sem_eval_2010_task_8, jonfd/ICC, julien-c/dummy-dataset-from-colab, julien-c/reactiongif, juny116/few_glue, k-halid/ar, k0t1k/test, kaka10/fgfgfgfg, karinev/lanuitdudroit, katoensp/VR-OP, keshan/clean-si-mc4, keshan/large-sinhala-asr-dataset, keshan/multispeaker-tts-sinhala, keshan/wit-dataset, kevinlu1248/personificationgen, khanbaba/online_love, kiamehr74/CoarseWSD-20, kleinay/qa_srl2018, kleinay/qanom, kmfoda/booksum, kmyoo/klue-tc-dev, knilakshan20/wikigold, laugustyniak/abusive-clauses-pl, lavis-nlp/german_legal_sentences, lbourdois/CoFiF, leoapolonio/AMI_Meeting_Corpus, lewtun/asr-preds-test, lewtun/asr_dummy, lewtun/binary_classification_dummy, lewtun/bulk-superb-s3p-superb-49606, lewtun/drug-reviews, lewtun/github-issues-test, lewtun/github-issues, lewtun/mnist-preds, lewtun/s3prl-sd-dummy, lewtun/text_classification_dummy, lhoestq/custom_squad, lhoestq/demo1, lhoestq/squad, lhoestq/test, lhoestq/test2, lhoestq/wikipedia_bn, liam168/nlp_c4_sentiment, lkiouiou/o9ui7877687, lkndsjkndgskjngkjsndkj/jsjdjsdvkjvszlhdskb, lohanna/testedjkcxkf, lorsorlah/Dadedadedam, loveguruji609/dfdfsdfsdfsdfsdfsd, lucien/sciencemission, lucien/voacantonesed, lucien/wsaderfffjjjhhh, lucio/common_voice_eval, lukasmasuch/my-test-repo-3, lukasmasuch/my-test-repo-4, lukasmasuch/test-2, lukasmasuch/test-3, lukasmasuch/test, luofengge/mydata, luofengge/testDataset, lvwerra/codeparrot-valid, m3hrdadfi/recipe_nlg_lite, mad/IndonesiaNewsDataset, maji/npo_mission_statement_ucf, majod/CleanNaturalQuestionsDataset, makanan/umich, manandey/OSCAR_Entity_Toy, markscrivo/OddsOn, martodaniel/terere, medzaf/test, merve/folk-mythology-tales, merve/poetry, metalearning/kaggale-nlp-tutorial, mhtoin/register_oscar, mksaad/Arabic_news, ml6team/cnn_dailymail_nl, mldmm/glass_alloy_composition, mmm-da/rutracker_anime_torrent_titles, mr-robot/ec, mrojas/abbreviation, mrojas/body, mrojas/disease, mrojas/family, mrojas/finding, mrojas/medication, mrojas/procedure, mulcyber/europarl-mono, munggok/mc4-id, mustafa12/db_ee, mustafa12/edaaaas, mustafa12/thors, nateraw/auto-cats-and-dogs, nateraw/auto-exp-2, nateraw/beans, nateraw/beans_old, nateraw/bulk-dummy, nateraw/cats-and-dogs, nateraw/cats_vs_dogs, nateraw/fairface, nateraw/filings-10k, nateraw/food101, nateraw/food101_old, nateraw/image-folder, nateraw/imagefolder, nateraw/imagenette, nateraw/img-demo, nateraw/rock_paper_scissors, nateraw/sync_food101, nateraw/test, nateraw/wit, nathanlsl/news, naver-clova-conversation/klue-tc-dev-tsv, naver-clova-conversation/klue-tc-tsv, naver-clova-conversation-ul/klue-tc-dev, nbroad/few-nerd, ncoop57/rico_captions, neelalex/raft-predictions, nielsr/FUNSD_layoutlmv2, nielsr/XFUN, nielsr/funsd, nlpaueb/test, nlpufg/brwac-pt, nlpufg/brwac, nlpufg/oscar-pt, notional/notional-python, nucklehead/ht-voice-dataset, oelkrise/CRT, osanseviero/codeparrot-train, osanseviero/llama_test, osanseviero/test, ought/raft-submission, ought/raft, pariajm/sharif_emotional_speech_dataset, parivartanayurveda/Malesexproblemsayurvedictreatment, pasinit/scotus, pasinit/xlwic, patrickvonplaten/common_voice_processed_turkish, patrickvonplaten/librispeech_asr_dummy, patrickvonplaten/librispeech_local, patrickvonplaten/librispeech_local_dummy, patrickvonplaten/scientific_papers_dummy, pdesoyres/test, peixian/equity_evaluation_corpus, peixian/rtGender, pelican/test_100, pere/norwegian_colossal_corpus_v2_short100k, persiannlp/parsinlu_entailment, persiannlp/parsinlu_query_paraphrasing, persiannlp/parsinlu_reading_comprehension, persiannlp/parsinlu_sentiment, persiannlp/parsinlu_translation_en_fa, persiannlp/parsinlu_translation_fa_en, phonlab-tcd/cngv1, phonlab-tcd/corpuscrawler-ga, piEsposito/br-quad-2.0, piEsposito/br_quad_20, piEsposito/squad_20_ptbr, pierreant-p/jcvd-or-linkedin, princeton-nlp/datasets-for-simcse, priya3301/Graduation_admission, priya3301/tes, priya3301/test, proffttega/ILLUMINATI, proffttega/doc, proffttega/join_illuminati_to_become_rich, proffttega/persian_daily_news, pulmo/chest_xray, qfortier/instagram_ny, qwant/squad_fr, rahular/itihasa, rajeshradhakrishnan/malayalam_2020_wiki, rajeshradhakrishnan/malayalam_news, ramybaly/conll2012, ramybaly/nerd, ranpox/xfund, rays2pix/example, rays2pix/example_dataset, rewardsignal/reddit_writing_prompts, rom1504/laion400m, rony/soccer-dialogues, roskoN/dailydialog, roskoN/dstc8-reddit-corpus, s-myk/test, sagnikrayc/mctest, sagnikrayc/quasar, salesken/Paraphrase_category_detection, sanyu/aw, sanyu/er, sanyu/hh, sanyu/vb, sc2qa/sc2q_commoncrawl, sc2qa/sc2qa_commoncrawl, sdfufygvjh/fgghuviugviu, seamew/ChnSentiCorp, seamew/Hotel, seamew/THUCNews, seamew/THUCNewsText, seamew/THUCNewsTitle, seamew/Weibo, seanbethard/autonlp-data-summarization_model, sentence-transformers/embedding-training-data, sentence-transformers/msmarco-hard-negatives, sentence-transformers/parallel-sentences, sentence-transformers/reddit-title-body, seregadgl/test_set, sevbqewre/vebdesbdty, severo/autonlp-data-sentiment_detection-3c8bcd36, shahrukhx01/questions-vs-statements, shanya/website_metadata_c4_toy, sharejing/BiPaR, sijpapi/batch13, sijpapi/funsd, sijpapi/funsds, sileod/metaeval, sismetanin/rureviews, smallv0221/my-test, somaimanguyat/Genjer, somaimanguyat/Koboy, somaimanguyat/Movieonline2021, somaimanguyat/Salome, somaimanguyat/movie21, somaimanguyat/xiomay, spacemanidol/ms_marco_doc2query, spacemanidol/msmarco_passage_ranking, ssasaa/gghghgh, sshleifer/pseudo_bart_xsum, stas/c4-en-10k, stas/openwebtext-10k, stas/oscar-en-10k, stas/wmt14-en-de-pre-processed, stas/wmt16-en-ro-pre-processed, stevhliu/demo, stiel/skjdhjkasdhasjkd, subiksha/OwnDataset, superb/superb-data, susumu2357/squad_v2_sv, svalabs/all-nli-german-translation-wmt19, svalabs/ms-marco-german-translation-wmt19, svanhvit/iceErrorCorpus, tals/test, tanfiona/causenet_wiki, tarudesu/UIT-ViCTSD, tharindu/MOLD, thiemowa/argumentationreviewcorpus, thiemowa/empathyreviewcorpus, thomwolf/codeparrot-train, thomwolf/codeparrot-valid, thomwolf/codeparrot, thomwolf/github-dataset, thomwolf/github-python, thomwolf/very-good-dataset, thomwolf/very-test-dataset-2, thomwolf/very-test-dataset, tianxing1994/temp, toloka/CrowdSpeech, toloka/VoxDIY-RusNews, tommy19970714/common_voice, toriving/kosimcse, toriving/talktalk-sentiment-210713-multi-singleturn-custom-multiturn, transformersbook/codeparrot-train, transformersbook/codeparrot-valid, transformersbook/codeparrot, ttj/metadata_arxiv, turingbench/TuringBench, uasoyasser/rgfes, usc-isi/Wiki-Convert, uva-irlab/canard_quretec, uva-irlab/trec-cast-2019-multi-turn, uyeongjae/load_klue_re_agmented, vasudevgupta/amazon-ml-hack, vasudevgupta/bigbird-tokenized-natural-questions, vasudevgupta/data, vasudevgupta/gsoc-librispeech, vasudevgupta/natural-questions-validation, vasudevgupta/temperature-distribution-2d-plate, vasudevgupta/temperature-distribution-3d-cylinder, vblagoje/eli5, vblagoje/wikipedia_snippets_streamed, vctc92/sdsd, vctc92/test, versae/norwegian-t5-dataset-debug, versae/norwegian-t5-dataset-debug2, versae/norwegian-t5-dataset-debug3, vershasaxena91/datasets, vershasaxena91/squad_multitask, vesteinn/icelandic-ner-MIM-GOLD-NER, w-nicole/childes_data, w-nicole/childes_data_no_tags, w-nicole/childes_data_no_tags_, w-nicole/childes_data_with_tags, w-nicole/childes_data_with_tags_, w11wo/imdb-javanese, webek18735/ddvoacantonesed, webek18735/dhikhscook, webis/args_me, weijieliu/senteval_cn, wisdomify/story, wmt/europarl, wmt/news-commentary, wmt/uncorpus, wmt/wikititles, wmt/wmt10, wmt/wmt13, wmt/wmt14, wmt/wmt15, wmt/wmt16, wmt/wmt17, wmt/wmt18, wmt/wmt19, wzkariampuzha/EpiExtract4GARD, xiaj/ds_test, xiaj/test0919, yannobla/Sunshine, yazdipour/text-to-sparql-t5-lc-quad-v2, yluisfern/PBU, yo/devparty, yuanchuan/annotated_reference_strings\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "cola_dataset = load_dataset(\"glue\", \"cola\")\r\n",
    "print(cola_dataset)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset glue (C:\\Users\\jhseo\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 501.27it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 8551\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1043\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['sentence', 'label', 'idx'],\n",
      "        num_rows: 1063\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "train_dataset = cola_dataset['train']\r\n",
    "print(train_dataset[0])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\", 'label': 1, 'idx': 0}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "class DataModule(pl.LightningDataModule):\r\n",
    "    def __init__(self, model_name=\"google/bert_uncased_L-2_H-128_A-2\", batch_size=32):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        self.batch_size = batch_size\r\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n",
    "\r\n",
    "    def prepare_data(self):\r\n",
    "        cola_dataset = load_dataset(\"glue\", \"cola\")\r\n",
    "        self.train_data = cola_dataset[\"train\"]\r\n",
    "        self.val_data = cola_dataset[\"validation\"]\r\n",
    "\r\n",
    "    def tokenize_data(self, example):\r\n",
    "        # processing the data\r\n",
    "        return self.tokenizer(\r\n",
    "            example[\"sentence\"],\r\n",
    "            truncation=True,\r\n",
    "            padding=\"max_length\",\r\n",
    "            max_length=256,\r\n",
    "        )\r\n",
    "\r\n",
    "    def setup(self, stage=None):\r\n",
    "        if stage == \"fit\" or stage is None:\r\n",
    "            self.train_data = self.train_data.map(self.tokenize_data, batched=True)\r\n",
    "            self.train_data.set_format(\r\n",
    "                type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\r\n",
    "            )\r\n",
    "\r\n",
    "            self.val_data = self.val_data.map(self.tokenize_data, batched=True)\r\n",
    "            self.val_data.set_format(\r\n",
    "                type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]\r\n",
    "            )\r\n",
    "\r\n",
    "    def train_dataloader(self):\r\n",
    "        return torch.utils.data.DataLoader(\r\n",
    "            self.train_data, batch_size=self.batch_size, shuffle=True\r\n",
    "        )\r\n",
    "\r\n",
    "    def val_dataloader(self):\r\n",
    "        return torch.utils.data.DataLoader(\r\n",
    "            self.val_data, batch_size=self.batch_size, shuffle=False\r\n",
    "        )\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "class ColaModel(pl.LightningModule):\r\n",
    "    def __init__(self, model_name=\"google/bert_uncased_L-2_H-128_A-2\", lr=1e-2):\r\n",
    "        super(ColaModel, self).__init__()\r\n",
    "        self.save_hyperparameters()\r\n",
    "\r\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\r\n",
    "        self.W = nn.Linear(self.bert.config.hidden_size, 2)\r\n",
    "        self.num_classes = 2\r\n",
    "\r\n",
    "    def forward(self, input_ids, attention_mask):\r\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\r\n",
    "\r\n",
    "        h_cls = outputs.last_hidden_state[:, 0]\r\n",
    "        logits = self.W(h_cls)\r\n",
    "        return logits\r\n",
    "\r\n",
    "    def training_step(self, batch, batch_idx):\r\n",
    "        logits = self.forward(batch[\"input_ids\"], batch[\"attention_mask\"])\r\n",
    "        loss = F.cross_entropy(logits, batch[\"label\"])\r\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\r\n",
    "        return loss\r\n",
    "\r\n",
    "    def validation_step(self, batch, batch_idx):\r\n",
    "        logits = self.forward(batch[\"input_ids\"], batch[\"attention_mask\"])\r\n",
    "        loss = F.cross_entropy(logits, batch[\"label\"])\r\n",
    "        _, preds = torch.max(logits, dim=1)\r\n",
    "        val_acc = accuracy_score(preds.cpu(), batch[\"label\"].cpu())\r\n",
    "        val_acc = torch.tensor(val_acc)\r\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\r\n",
    "        self.log(\"val_acc\", val_acc, prog_bar=True)\r\n",
    "\r\n",
    "    def configure_optimizers(self):\r\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams[\"lr\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "cola_data = DataModule()\r\n",
    "cola_model = ColaModel()\r\n",
    "\r\n",
    "checkpoint_callback = ModelCheckpoint(\r\n",
    "    dirpath=\"./models\", monitor=\"val_loss\", mode=\"min\"\r\n",
    ")\r\n",
    "\r\n",
    "early_stopping_callback = EarlyStopping(\r\n",
    "    monitor=\"val_loss\", patience=3, verbose=True, mode=\"min\"\r\n",
    ")\r\n",
    "\r\n",
    "trainer = pl.Trainer(\r\n",
    "    default_root_dir=\"logs\",\r\n",
    "    gpus=(1 if torch.cuda.is_available() else 0),\r\n",
    "    max_epochs=1,\r\n",
    "    # fast_dev_run=True,\r\n",
    "    logger=pl.loggers.TensorBoardLogger(\"logs/\", name=\"cola\", version=1),\r\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\r\n",
    ")\r\n",
    "trainer.fit(cola_model, cola_data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\ProgramData\\Anaconda3\\envs\\mlops\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:446: UserWarning: Checkpoint directory ./models exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Reusing dataset glue (C:\\Users\\jhseo\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|██████████| 3/3 [00:00<00:00, 501.35it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 12.29ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 19.28ba/s]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type      | Params\n",
      "-----------------------------------\n",
      "0 | bert | BertModel | 4.4 M \n",
      "1 | W    | Linear    | 258   \n",
      "-----------------------------------\n",
      "4.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.4 M     Total params\n",
      "17.545    Total estimated model params size (MB)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                                                              "
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\mlops\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\ProgramData\\Anaconda3\\envs\\mlops\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0: 100%|██████████| 301/301 [00:09<00:00, 30.76it/s, loss=0.634, v_num=1, train_loss=0.756, val_loss=0.618, val_acc=0.691]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Metric val_loss improved. New best score: 0.618\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0: 100%|██████████| 301/301 [00:09<00:00, 30.23it/s, loss=0.634, v_num=1, train_loss=0.756, val_loss=0.618, val_acc=0.691]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "class ColaPredictor:\r\n",
    "    def __init__(self, model_path):\r\n",
    "        self.model_path = model_path\r\n",
    "        # loading the trained model\r\n",
    "        self.model = ColaModel.load_from_checkpoint(model_path)\r\n",
    "        # keep the model in eval mode\r\n",
    "        self.model.eval()\r\n",
    "        self.model.freeze()\r\n",
    "        self.processor = DataModule()\r\n",
    "        self.softmax = torch.nn.Softmax(dim=0)\r\n",
    "        self.lables = [\"unacceptable\", \"acceptable\"]\r\n",
    "\r\n",
    "    def predict(self, text):\r\n",
    "        # text => run time input\r\n",
    "        inference_sample = {\"sentence\": text}\r\n",
    "        # tokenizing the input\r\n",
    "        processed = self.processor.tokenize_data(inference_sample)\r\n",
    "        # predictions\r\n",
    "        logits = self.model(\r\n",
    "            torch.tensor([processed[\"input_ids\"]]),\r\n",
    "            torch.tensor([processed[\"attention_mask\"]]),\r\n",
    "        )\r\n",
    "        scores = self.softmax(logits[0]).tolist()\r\n",
    "        predictions = []\r\n",
    "        for score, label in zip(scores, self.lables):\r\n",
    "            predictions.append({\"label\": label, \"score\": score})\r\n",
    "        return predictions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "sentence = \"The boy is sitting on a bench\"\r\n",
    "predictor = ColaPredictor(\"./models/epoch=0-step=267.ckpt\")\r\n",
    "print(predictor.predict(sentence))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'label': 'unacceptable', 'score': 0.21141359210014343}, {'label': 'acceptable', 'score': 0.7885863780975342}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('mlops': conda)"
  },
  "interpreter": {
   "hash": "8e45873a4542e3cc3866bc5e6205c41a5b64d8a4fb25e1bd2147562e54ef506d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}